{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMv5q40dqqD/S3Zo58OQiRV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dom557/Fine-Tuning-gpt2/blob/main/finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Install Libraries ---\n",
        "!pip install  transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LWKY2VawDFLO",
        "outputId": "6b33df92-65ca-4c7b-c981-be48ce64b913"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Install necessary libraries\n",
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWd8A0cxbOOb",
        "outputId": "dec63127-9e98-49db-f2d5-a837666206d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFSEfb14DiDL",
        "outputId": "3eda3977-048a-4738-9f3d-e88df8ce198a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability and specifications\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTweSDbwZtRB",
        "outputId": "978a828a-7399-497a-a26e-9112d2449654"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 29 11:55:26 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "eCll_Ka5CD49",
        "outputId": "565384d1-d58f-45f2-9359-74c22d70d191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6/90 00:03 < 01:18, 1.06 it/s, Epoch 0.56/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1e6de81f2401>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m# ✅ Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_loss\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training_loss'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"N/A\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Final training loss: {training_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2563\u001b[0m                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m                         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_xla_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2565\u001b[0;31m                         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2566\u001b[0m                     ):\n\u001b[1;32m   2567\u001b[0m                         \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ✅ Imports\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# ✅ Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ✅ Create output directory with timestamp\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_dir = f\"./gpt2-finetuned_{timestamp}\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# ✅ Load tokenizer and model (using standard GPT2 instead of tiny version)\n",
        "model_name = \"gpt2\"  # Much better than tiny-gpt2\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# ✅ Ensure pad token exists\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# ✅ Load and split your training dataset\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"train.txt\"})\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)  # 10% validation set\n",
        "\n",
        "# ✅ Tokenize the dataset with larger context window\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128  # Increased from 64 for better context\n",
        "    )\n",
        "\n",
        "# ✅ Process datasets\n",
        "tokenized_datasets = {}\n",
        "for split in [\"train\", \"test\"]:\n",
        "    tokenized_datasets[split] = dataset[split].map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=[\"text\"],\n",
        "        desc=f\"Tokenizing {split} dataset\"\n",
        "    )\n",
        "    logger.info(f\"{split} dataset size: {len(tokenized_datasets[split])}\")\n",
        "\n",
        "# ✅ Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# ✅ Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "logger.info(f\"Training on: {device}\")\n",
        "\n",
        "# ✅ Create training arguments with improved hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    # Increased training time\n",
        "    num_train_epochs=10,  # Train longer (up from 3)\n",
        "\n",
        "    # Optimized batch size\n",
        "    per_device_train_batch_size=8,  # Larger batch size (up from 4)\n",
        "    gradient_accumulation_steps=8,  # For effective batch size of 64\n",
        "\n",
        "    # Learning rate schedule\n",
        "    learning_rate=5e-5,  # Optimized learning rate\n",
        "    warmup_steps=500,  # Warm up learning rate\n",
        "\n",
        "    # Regularization\n",
        "    weight_decay=0.01,  # Prevent overfitting\n",
        "\n",
        "    # Evaluation and checkpointing\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,  # Save disk space\n",
        "\n",
        "    # Mixed precision training\n",
        "    fp16=torch.cuda.is_available(),  # Use half precision on GPU\n",
        "\n",
        "    # Mitigate RAM issues\n",
        "    dataloader_num_workers=4,  # Parallel data loading\n",
        "\n",
        "    # Output all training logs\n",
        "    report_to=\"all\",\n",
        ")\n",
        "\n",
        "# ✅ Initialize Trainer with validation dataset\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# ✅ Start training\n",
        "logger.info(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "training_loss = train_result.training_loss if hasattr(train_result, 'training_loss') else \"N/A\"\n",
        "logger.info(f\"Final training loss: {training_loss}\")\n",
        "\n",
        "# ✅ Save final model\n",
        "trainer.save_model(os.path.join(output_dir, \"final_model\"))\n",
        "tokenizer.save_pretrained(os.path.join(output_dir, \"final_model\"))\n",
        "logger.info(f\"Training complete. Model saved to {os.path.join(output_dir, 'final_model')}\")\n",
        "\n",
        "# ✅ Evaluate final loss on validation set manually\n",
        "logger.info(\"Evaluating on validation set...\")\n",
        "model.eval()\n",
        "validation_dataloader = torch.utils.data.DataLoader(\n",
        "    tokenized_datasets[\"test\"],\n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "total_loss = 0\n",
        "total_samples = 0\n",
        "with torch.no_grad():\n",
        "    for batch in validation_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item() * len(batch[\"input_ids\"])\n",
        "        total_samples += len(batch[\"input_ids\"])\n",
        "\n",
        "avg_validation_loss = total_loss / total_samples\n",
        "logger.info(f\"Final validation loss: {avg_validation_loss:.4f}\")\n",
        "\n",
        "# ✅ Optional: Generate sample text to test the model\n",
        "if torch.cuda.is_available():  # Only do this on GPU to save time\n",
        "    prompt = \"Once upon a time\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    logger.info(f\"Sample generated text:\\n{generated_text}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# This is formatted as code\n",
        "\n",
        "\n",
        "# ✅ Recipe-specific testing script for your fine-tuned GPT-2 model\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# ✅ Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create output directory for test results\n",
        "test_output_dir = f\"./model_test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "os.makedirs(test_output_dir, exist_ok=True)\n",
        "\n",
        "# ✅ Path to your fine-tuned model\n",
        "# Replace with the actual path to your saved model\n",
        "model_path = \"./gpt2-finetuned_YYYYMMDD_HHMMSS/final_model\"\n",
        "if not os.path.exists(model_path):\n",
        "    logger.warning(f\"Model path {model_path} not found. Please update with your actual model path.\")\n",
        "    # List available model directories to help the user\n",
        "    directories = [d for d in os.listdir('./') if d.startswith('gpt2-finetuned_') and os.path.isdir(d)]\n",
        "    if directories:\n",
        "        logger.info(f\"Available model directories: {directories}\")\n",
        "        model_path = directories[-1] + \"/final_model\"  # Use the most recent one\n",
        "        logger.info(f\"Using the most recent model: {model_path}\")\n",
        "\n",
        "# ✅ Load tokenizer and model\n",
        "try:\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    logger.info(f\"Successfully loaded model from {model_path}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error loading model: {e}\")\n",
        "    logger.info(\"Falling back to original GPT-2 model\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# ✅ Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ Set padding token to avoid warnings\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# ✅ Testing functions\n",
        "def generate_recipe(goal_prompt, max_length=300, temperature=0.7):\n",
        "    \"\"\"Generate a recipe based on a nutritional goal\"\"\"\n",
        "    # Create the full prompt\n",
        "    full_prompt = f\"### Goal: {goal_prompt}\\n### Recipe: \"\n",
        "\n",
        "    # Encode the prompt\n",
        "    input_ids = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            no_repeat_ngram_size=2,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract just the recipe part (remove the original prompt)\n",
        "    recipe_part = generated_text[len(full_prompt):]\n",
        "\n",
        "    # Clean up any incomplete recipes (stop at the next goal if present)\n",
        "    if \"### Goal:\" in recipe_part:\n",
        "        recipe_part = recipe_part.split(\"### Goal:\")[0].strip()\n",
        "\n",
        "    return full_prompt + recipe_part\n",
        "\n",
        "def parse_recipe(recipe_text):\n",
        "    \"\"\"Parse a recipe to extract structured information\"\"\"\n",
        "    recipe_data = {\n",
        "        \"title\": \"\",\n",
        "        \"ingredients\": [],\n",
        "        \"instructions\": [],\n",
        "        \"nutrition\": {}\n",
        "    }\n",
        "\n",
        "    # Extract title\n",
        "    title_match = re.search(r\"Title: (.*?)(?:\\n|$)\", recipe_text)\n",
        "    if title_match:\n",
        "        recipe_data[\"title\"] = title_match.group(1).strip()\n",
        "\n",
        "    # Extract ingredients\n",
        "    ingredients_section = re.search(r\"Ingredients:(.*?)Instructions:\", recipe_text, re.DOTALL)\n",
        "    if ingredients_section:\n",
        "        ingredients_text = ingredients_section.group(1).strip()\n",
        "        ingredients_list = [item.strip().lstrip('- ') for item in ingredients_text.split('\\n') if item.strip()]\n",
        "        recipe_data[\"ingredients\"] = ingredients_list\n",
        "\n",
        "    # Extract instructions\n",
        "    instructions_section = re.search(r\"Instructions:(.*?)(?:Nutrition:|$)\", recipe_text, re.DOTALL)\n",
        "    if instructions_section:\n",
        "        instructions_text = instructions_section.group(1).strip()\n",
        "        instructions_list = []\n",
        "        for line in instructions_text.split('\\n'):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                # Remove numbering if present\n",
        "                if re.match(r\"^\\d+\\.\\s\", line):\n",
        "                    line = re.sub(r\"^\\d+\\.\\s\", \"\", line)\n",
        "                instructions_list.append(line.strip())\n",
        "        recipe_data[\"instructions\"] = instructions_list\n",
        "\n",
        "    # Extract nutrition\n",
        "    nutrition_section = re.search(r\"Nutrition: (.*?)(?:\\n|$)\", recipe_text)\n",
        "    if nutrition_section:\n",
        "        nutrition_text = nutrition_section.group(1).strip()\n",
        "        # Parse calories, sugar, protein, fat\n",
        "        calories_match = re.search(r\"(\\d+)\\s*calories\", nutrition_text)\n",
        "        sugar_match = re.search(r\"(\\d+)g\\s*sugar\", nutrition_text)\n",
        "        protein_match = re.search(r\"(\\d+)g\\s*protein\", nutrition_text)\n",
        "        fat_match = re.search(r\"(\\d+)g\\s*fat\", nutrition_text)\n",
        "\n",
        "        if calories_match:\n",
        "            recipe_data[\"nutrition\"][\"calories\"] = int(calories_match.group(1))\n",
        "        if sugar_match:\n",
        "            recipe_data[\"nutrition\"][\"sugar\"] = int(sugar_match.group(1))\n",
        "        if protein_match:\n",
        "            recipe_data[\"nutrition\"][\"protein\"] = int(protein_match.group(1))\n",
        "        if fat_match:\n",
        "            recipe_data[\"nutrition\"][\"fat\"] = int(fat_match.group(1))\n",
        "\n",
        "    return recipe_data\n",
        "\n",
        "def evaluate_recipe_quality(recipe_data, goal):\n",
        "    \"\"\"Evaluate the quality of a generated recipe\"\"\"\n",
        "    evaluation = {\n",
        "        \"has_title\": bool(recipe_data[\"title\"]),\n",
        "        \"ingredient_count\": len(recipe_data[\"ingredients\"]),\n",
        "        \"instruction_count\": len(recipe_data[\"instructions\"]),\n",
        "        \"has_nutrition\": bool(recipe_data[\"nutrition\"]),\n",
        "        \"matches_goal\": False,\n",
        "        \"overall_score\": 0  # Will be calculated\n",
        "    }\n",
        "\n",
        "    # Check if nutrition information matches the goal\n",
        "    if recipe_data[\"nutrition\"] and \"calories\" in recipe_data[\"nutrition\"]:\n",
        "        # Extract target calories from goal\n",
        "        calories_match = re.search(r\"around (\\d+) calories\", goal)\n",
        "        if calories_match:\n",
        "            target_calories = int(calories_match.group(1))\n",
        "            actual_calories = recipe_data[\"nutrition\"][\"calories\"]\n",
        "            # Check if calories are within 20% of target\n",
        "            if abs(actual_calories - target_calories) <= (target_calories * 0.2):\n",
        "                evaluation[\"matches_goal\"] = True\n",
        "\n",
        "    # Calculate overall score (simple heuristic)\n",
        "    score = 0\n",
        "    if evaluation[\"has_title\"]:\n",
        "        score += 1\n",
        "    if evaluation[\"ingredient_count\"] >= 3:\n",
        "        score += 1\n",
        "    if evaluation[\"instruction_count\"] >= 2:\n",
        "        score += 1\n",
        "    if evaluation[\"has_nutrition\"]:\n",
        "        score += 1\n",
        "    if evaluation[\"matches_goal\"]:\n",
        "        score += 1\n",
        "\n",
        "    evaluation[\"overall_score\"] = score\n",
        "    return evaluation\n",
        "\n",
        "# ✅ Main testing function\n",
        "def run_recipe_tests():\n",
        "    \"\"\"Run comprehensive tests on the recipe generation model\"\"\"\n",
        "    # Test prompts based on the training data format\n",
        "    test_goals = [\n",
        "        \"around 500 calories, high protein, moderate fat\",\n",
        "        \"around 350 calories, low carb, high fat\",\n",
        "        \"around 400 calories, high fiber, low sugar\",\n",
        "        \"around 600 calories, high carb, low fat\",\n",
        "        \"around 450 calories, balanced macros\",\n",
        "        \"around 300 calories, high protein, very low sugar\"\n",
        "    ]\n",
        "\n",
        "    # Store test results\n",
        "    all_results = []\n",
        "\n",
        "    # Run tests for each goal\n",
        "    for goal in test_goals:\n",
        "        logger.info(f\"\\nGenerating recipe for goal: {goal}\")\n",
        "\n",
        "        # Generate recipe\n",
        "        recipe_text = generate_recipe(goal)\n",
        "        logger.info(f\"Generated recipe:\\n{recipe_text}\\n\")\n",
        "\n",
        "        # Save to file\n",
        "        recipe_filename = os.path.join(test_output_dir, f\"recipe_{goal.replace(' ', '_')[:20]}.txt\")\n",
        "        with open(recipe_filename, \"w\") as f:\n",
        "            f.write(recipe_text)\n",
        "\n",
        "        # Parse and evaluate recipe\n",
        "        recipe_data = parse_recipe(recipe_text)\n",
        "        evaluation = evaluate_recipe_quality(recipe_data, goal)\n",
        "\n",
        "        # Log evaluation\n",
        "        logger.info(f\"Recipe evaluation: {evaluation}\")\n",
        "\n",
        "        # Add to results\n",
        "        result = {\n",
        "            \"goal\": goal,\n",
        "            \"recipe_text\": recipe_text,\n",
        "            \"parsed_recipe\": recipe_data,\n",
        "            \"evaluation\": evaluation\n",
        "        }\n",
        "        all_results.append(result)\n",
        "\n",
        "    # Save overall results\n",
        "    with open(os.path.join(test_output_dir, \"test_results.json\"), \"w\") as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "\n",
        "    # Calculate average score\n",
        "    avg_score = sum(r[\"evaluation\"][\"overall_score\"] for r in all_results) / len(all_results)\n",
        "    logger.info(f\"\\nAverage recipe quality score: {avg_score:.2f}/5\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# ✅ Interactive mode for recipe generation\n",
        "def interactive_recipe_generation():\n",
        "    \"\"\"Interactive mode for testing the recipe generator\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"INTERACTIVE RECIPE GENERATOR\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Enter your nutritional goals, or type 'quit' to exit\")\n",
        "    print(\"Example: 'around 400 calories, high protein, low fat'\")\n",
        "\n",
        "    while True:\n",
        "        user_goal = input(\"\\nYour nutritional goal: \")\n",
        "        if user_goal.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        print(\"\\nGenerating recipe...\")\n",
        "        recipe_text = generate_recipe(user_goal)\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(recipe_text)\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        # Save the generated recipe\n",
        "        timestamp = datetime.now().strftime(\"%H%M%S\")\n",
        "        recipe_filename = os.path.join(test_output_dir, f\"interactive_recipe_{timestamp}.txt\")\n",
        "        with open(recipe_filename, \"w\") as f:\n",
        "            f.write(recipe_text)\n",
        "        print(f\"Recipe saved to {recipe_filename}\")\n",
        "\n",
        "# ✅ Run tests\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"RECIPE MODEL TESTING\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"1: Run automated tests\")\n",
        "    print(\"2: Interactive recipe generation\")\n",
        "    print(\"3: Run both\")\n",
        "\n",
        "    choice = input(\"\\nEnter your choice (1-3): \")\n",
        "\n",
        "    if choice in [\"1\", \"3\"]:\n",
        "        print(\"\\nRunning automated tests...\")\n",
        "        results = run_recipe_tests()\n",
        "        print(f\"\\nTest results saved to {test_output_dir}\")\n",
        "\n",
        "    if choice in [\"2\", \"3\"]:\n",
        "        interactive_recipe_generation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6dm8meetrZSt",
        "outputId": "e03f1033-a794-448b-c87e-d5aab495f241"
      },
      "execution_count": 36,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Model path ./gpt2-finetuned_YYYYMMDD_HHMMSS/final_model not found. Please update with your actual model path.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "RECIPE MODEL TESTING\n",
            "==================================================\n",
            "1: Run automated tests\n",
            "2: Interactive recipe generation\n",
            "3: Run both\n",
            "\n",
            "Running automated tests...\n",
            "\n",
            "Test results saved to ./model_test_results_20250429_132437\n",
            "\n",
            "==================================================\n",
            "INTERACTIVE RECIPE GENERATOR\n",
            "==================================================\n",
            "Enter your nutritional goals, or type 'quit' to exit\n",
            "Example: 'around 400 calories, high protein, low fat'\n",
            "\n",
            "Generating recipe...\n",
            "\n",
            "--------------------------------------------------\n",
            "### Goal: around 700 calories , high sugar , low fat\n",
            "### Recipe:  Slow Cooker Low Carb Chicken Stew with Chickpea Salad with Vegetable Broccoli and Broiled Cilantro\n",
            "Print Low-Carb Chicken and Vegetables Stew Author: Gina Homework Ingredients 1 medium chicken breast\n",
            "\n",
            "1/2 cup olive oil\n",
            ".\n",
            "/ 3 cups broccoli, thinly sliced\n",
            "- 1 tbsp zucchini, finely chopped\n",
            ", or 3 tbsp romaine lettuce\n",
            "Cilantro, chopped Instructions Preheat oven to 350 degrees. In a large bowl, combine chicken and broccoli. Drain and serve. Recipe Notes Recipe may or may not be vegan. Nutrition Facts Low Fat Chicken & Vegetate Broiler Stew Amount Per Serving: 8 Amount per Serving : 5 Calories: 230 % Daily Value: 15g 2% * Percent Daily Values are based on a 2000 calorie diet.\n",
            "--------------------------------------------------\n",
            "Recipe saved to ./model_test_results_20250429_132437/interactive_recipe_132521.txt\n",
            "\n",
            "Generating recipe...\n",
            "\n",
            "--------------------------------------------------\n",
            "### Goal: something has low sugar\n",
            "### Recipe: iced tea, chocolate chip cookie, and coffee.\n",
            "Print Recipe Ingredients 1 cup all-purpose flour\n",
            "\n",
            "1 cup sweetened condensed milk\n",
            "\n",
            "\n",
            "\n",
            "2 cups sugar, plus more for serving\n",
            ": 1 tablespoon vanilla extract\n",
            ", or 1 medium banana, cut into cubes Instructions Preheat oven to 350 degrees. Mix together flour, sweetener, sugar and vanilla. Set aside. In a medium bowl, stir together milk, condensed and milk. Stir in banana and cream. Pour over chocolate chips. Bake for 10 minutes, until lightly golden brown. Serve immediately. Nutrition Serving size: about 2 cups\n",
            "*I made this with my mom's version of blueberries and cranberries. I used 3 cups of oats and 1/2 cup of sugar. For the chocolate cupcakes: add to milk and stir in sweeten.\n",
            "--------------------------------------------------\n",
            "Recipe saved to ./model_test_results_20250429_132437/interactive_recipe_132635.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-9c5442c69c39>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0minteractive_recipe_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-9c5442c69c39>\u001b[0m in \u001b[0;36minteractive_recipe_generation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0muser_goal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYour nutritional goal: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_goal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}